---
type: "docs"
title: "Sources"
linkTitle: "Sources"
weight: 40
description: >
    What are Sources and How to Use Them
---

Sources provide connectivity to the systems that Drasi can observe as sources of change. Sources perform three important functions within Drasi:
- Process the change log/feed generated by the source system and push those changes to each [Continuous Query](/solution-developer/components/continuous-queries) that uses that Source as input.
- Translate source change data into a consistent property graph data model so that subscribed Continuous Queries can use that data as if it where a graph of Nodes and Relations. For graph sources, such as Gremlin, no translation is necessary. But for non-graph sources, such as PostgreSQL and Kubernetes, the Source transforms the data (more detail is provided in the individual Sources sections below).
- Provide a way for Continuous Queries to query the source system at startup to initialize the state of the Continuous Query result.

{{< figure src="simple-end-to-end.png" alt="End to End" width="65%" >}}

Drasi currently provides Sources for the following source systems:

- [Azure Cosmos DB Gremlin API](#azure-cosmos-db-gremlin-api-source)
- [PostgreSQL](#postgresql-source)
- [Kubernetes](#kubernetes-source) (experimental)

## Creation
Sources can be created and managed using the `drasi` CLI. 

The easiest way to create a Source, and the way you will often create one as part of a broader software solution, is to:

1. Collect credentials and endpoint addresses that provide access to the change log and query API of the source system you want to connect to.
2. Create a YAML file containing the Source resource definition. This will include the configuration settings that enable the Source to connect to the source system. This file can be stored in your solution repo and versioned along with all the other solution code / resources.
3. Run `drasi apply` to apply the Source resource definition to the Kubernetes cluster where your Drasi environment is deployed.

As soon as the Source is created it will start running, monitoring its source system for changes and pushing them to subscribed Continuous Queries.

The definition for a Source has the following basic structure:

```
apiVersion: v1
kind: Source
name: <id>
spec:
  kind: <type>
  (source kind specific fields)...
```
The following table describes these configuration settings:

|Name|Description|
|-|-|
|apiVersion|Must have the value **v1**|
|kind|Must have the value **Source**|
|name|The **id** of the Source. Must be unique within the scope of the Sources in the Drasi deployment. The  **id** is used to identify the Source through the CLI/API and in a Continuous Query definitions to identify which Sources the Continuous Query subscribes to for change events.|
|spec.kind|The type of Source to create, which defines the type of database or source system the Source connects to. Must be one of [CosmosGremlin](#azure-cosmos-db-gremlin-api-source), [PostgreSQL](#postgresql-source) or [Kubernetes](#kubernetes-source).|
|spec.*|The configuration settings passed to the Source as name-value pairs. Properties differ depending on the Source type (**spec.kind**). See the individual Source sections below for the properties required by each Source type.  Any of these properties can either be specified inline or reference a secret. eg. 
```yaml
  kind: PostgreSQL
  user: my-user
  password:
    kind: Secret
    name: pg-creds
    key: password
```

Once configured, to create a Source defined in a file called `source.yaml`, you would run the command:

```
drasi apply -f source.yaml
```

You can then use the standard `drasi` commands to query the existence and status of the Source resource. For example, to see a list of the active Sources, run the following command:

```
drasi list source
```

## Deletion
To delete an active Source, run the following command:

```
drasi delete source <id>
```

For example, if the Source ID is `human-resources`, you would run,

```
drasi delete source human-resources
```

**Note**: Drasi does not currently enforce dependency integrity between Sources and Continuous Queries. If you delete a Source that is used by one or more Continuous Queries, they will stop getting change events and stop producing results.

## Configuring Sources
The following sections describe the configuration of the Source types currently supported by Drasi.

- [Azure Cosmos DB Gremlin API](#azure-cosmos-db-gremlin-api-source)
- [PostgreSQL](#postgresql-source)
- [Kubernetes](#kubernetes-source) (experimental)

### Azure Cosmos DB Gremlin API Source

The Azure Cosmos DB Gremlin API Source enables Drasi connectivity to Azure Cosmos DB Gremlin API. It uses the Cosmos DB Change Log as the source of database change events, and calls the Gremlin API to retrieve data required to bootstrap Continuous Queries at creation.

#### Source Requirements
For the Cosmos DB Gremlin Source to function, you must ensure the Full Fidelity Change Feed support is enabled on the Cosmos Account you intend to use. Currently (as of 02/03/2023), this needs to be manually requested by [filling out this form](https://forms.office.com/pages/responsepage.aspx?id=v4j5cvGGr0GRqy180BHbR9ecQmQM5J5LlXYOPoIbyzdUOFVRNUlLUlpRV0dXMjFRNVFXMDNRRjVDNy4u).

#### Configuration Settings
The following is an example of a full resource definition for an Azure Cosmos DB Gremlin API Source using Kubernetes Secrets to securely store database credentials:

```
apiVersion: v1
kind: Source
name: retail-ops
spec:
  kind: CosmosGremlin
  accountEndpoint: 
    kind: Secret
    name: creds
    key: account-endpoint
  database: Contoso
  container: RetailOperations
  partitionKey: name
```

> Note: You could use the following command to easily create the seret referenced here:
  ```bash
  kubectl create secret generic creds --from-literal=account-endpoint=...
  ```

In the Source resource definition:
- **apiVersion** must be **v1**
- **kind** must be **Source**
- **name** is the **id** of the Source and must be unique. This id is used in a Continuous Query definitions to identify which Sources the Continuous Query subscribes to for change events.
- **spec.kind** must be **CosmosGremlin**

The following table describes the Cosmos Gremlin specific properties that must be configured in the **spec** object:
|Property|Description|
|-|-|
|accountEndpoint|The **PRIMARY** or **SECONDARY CONNECTION STRING** from the **Keys** page of the Azure Cosmsos DB Account page of the Azure Portal.|
|database|**Database Id** from the Cosmos DB account.|
|container|**Graph Id** from the Cosmos DB Database.|
|partitionKey|The **Partition Key** configured on the **Graph**.|

#### Data Transformation
Cosmos DB Gremlin already uses a property graph data model and so the Source does not need to do any data transformation as it processes the inbound changes. The only thing to note is the terminology differences between Gremlin and Drasi summarized in this table:

|Gremlin Name|Drasi Name|
|-|-|
|Vertex|Node|
|Edge|Relation|

### PostgreSQL Source
The PostgreSQL Source enables Drasi connectivity to PostgreSQL databases. It uses the PostgreSQL replication log as the source of database change events, and calls the SQL API to retrieve data required to bootstrap Continuous Queries at creation.

#### Source Requirements

Your PostgreSQL database must be running at least version 10 and have `LOGICAL` replication enabled. See the notes on [configuring PostgreSQL replication](/reference/postgresql-replication) for assistance.

You also need a PostgreSQL user that has at least the LOGIN, REPLICATION and CREATE permissions on the database and SELECT permissions on the tables you are interested in.

#### Configuration Settings
The following is an example of a full resource definition for a PostgreSQL Source using Kubernetes Secrets to securely store database credentials:

```bash
kubectl create secret generic pg-creds --from-literal=password=my-password
```

```
apiVersion: v1
kind: Source
name: phys-ops
spec:
  kind: PostgreSQL
  host: reactive-graph.postgres.database.azure.com
  port: 5432
  user: postgres@reactive-graph
  password:
    kind: Secret
    name: creds
    key: password
  database: phys-ops
  ssl: true
  tables:
    - public.Vehicle
    - public.Zone
```

In the Source resource definition:
- **apiVersion** must be **v1**
- **kind** must be **Source**
- **name** is the **id** of the Source and must be unique. This id is used in a Continuous Query definitions to identify which Sources the Continuous Query subscribes to for change events.
- **spec.kind** must be **PostgreSQL**

The following table describes the PostgrSQL specific properties that must be configured in the **spec** object:
|Property|Description|
|-|-|
|host|The **host name** of the PostgreSQL database server.|
|port|The **port** number used to communicate with the PostgreSQL database server.|
|user|The **user id** to use for authentication against the PostgreSQL database server.|
|password|The **password** for the user account specified in the **user** property.|
|database|The name of the PostgreSQL database.|
|ssl|Does the server require a secure connection, valid values are "true" or "false".|
|tables| An array of table names that the Source should process changes for. Tables must have a **public.** prefix.|

#### Data Transformation
The PostgreSQL Source translates the relational data from change events to more closely resemble property graph data change events so that they can be processed by subscribed Continuous Queries. To achieve this, the PostgreSQL Source represents table rows as graph Nodes, as follows:
- Each row gets represented as a Node with the table columns as properties of the Node.
- The Node is assigned an id the is a composite of the table id and the row's primary key. This is Node metadata, not a property of the Node.
- The name of the table is assigned as a **Label** of the Node.

The PostgreSQL Source **does not** interpret foreign keys or joins from the relational source, instead relying on the Source Join feature provided by Continuous Queries to mimic graph-style Relations between Nodes based on the values of specified properties. See the [Source Joins](/solution-developer/components/continuous-queries/#source-subscriptions) topic in the [Continuous Queries](/solution-developer/components/continuous-queries) section for details. 

### Kubernetes Source
The Kubernetes Source is an early stage experimental Source that enables Drasi connectivity to Kubernetes clusters, enabling Drasi to support Continuous Queries that incorporate changes to Kubernetes resources.

#### Source Requirements

You will need a client side credentials that can be used to authenticate against your Kubernetes cluster and has permission to watch resources.

#### Configuration Settings
The following is an example of a full resource definition for a Kubernetes Source using Kubernetes Secrets to securely store credentials:

To get the credentials, export the Kubernetes credentials to a file named `credentials.yaml`

- For self hosted clusters, you can find this in your [kubeconfig](https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/) file
- For AKS, you can use this command
```
az aks get-credentials --resource-group <resource group> --name <cluster name> --file credentials.yaml
```

Create a secret named `k8s-context` from the `credentials.yaml` file

```bash
kubectl create secret generic k8s-context --from-file=credentials.yaml
```

```
apiVersion: v1
kind: Source
name: k8s
spec:
  kind: Kubernetes
  kubeconfig:
    kind: Secret
    name: k8s-context
    key: credentials.yaml
```

In the Source resource definition:
- **apiVersion** must be **v1**
- **kind** must be **Source**
- **name** is the **id** of the Source and must be unique. This id is used in a Continuous Query definitions to identify which Sources the Continuous Query subscribes to for change events.
- **spec.kind** must be **Kubernetes**

The following table describes the properties that must be configured in the **spec** object:
|Property|Description|
|-|-|
|kubeconfig|A [kubeconfig](https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig) containing the credentials to connect to your cluster|

#### Data Transformation

Currently, only Pods and Containers are projected onto a graph schema.  The relation between them is labeled `HOSTS` and flows from the `Pod` to the `Container`.  A MATCH cypher clause that connects these would look as follows

```cypher
MATCH (p:Pod)-[:HOSTS]->(c:Container) 
```

The following properties are projected to the graph nodes

|Node Label|Property|Origin|
|-|-|-|
|Container|name|Pod.status.containerStatuses[].name|
|Container|image|Pod.status.containerStatuses[].image|
|Container|started|Pod.status.containerStatuses[].started|
|Container|ready|Pod.status.containerStatuses[].ready|
|Container|restartCount|Pod.status.containerStatuses[].restartCount|
|Container|state|Pod.status.containerStatuses[].state|
|Container|message|Pod.status.containerStatuses[].state.message|
|Container|reason|Pod.status.containerStatuses[].state.reason|
|Container|terminationMessage|Pod.status.containerStatuses[].lastState.terminated.message|
|Pod|name|Pod.metadata.name|
|Pod|podIP|Pod.status.podIP|
|Pod|phase|Pod.status.phase|
|Pod|message|Pod.status.message|
|Pod|hostIP|Pod.status.hostIP|
|Pod|reason|Pod.status.reason|


## Creating the Source proxy and reactivator
{{< figure src="source.jpg" alt="Source" width="65%" >}}
### Source proxy
-	The Source proxy is responsible for bootstrapping the initial data when a continuous query is deployed. 
-	For any source proxy, we need to set up an HTTP server that listens for a POST request on the route ‘/acquire’. The bootstrap logic will be implemented under this section. As a result, when a POST request is received, the initial data will be obtained based on the logic here. 
    - For example, the SQL proxy uses knex to query and retrieve all of the rows in a SQL table and use the data as the bootstrapping data.
    - It is also worth noting that the passthru proxy might be useful in certain situations. The passthru proxy handles the bootstrapping process by using dapr to invoke a HTTP Post method called “acquire”, which is defined in the Source reactivator.

Sample code block:
```js
require("dotenv").config();

const fs = require('fs');
const dapr =  require("@dapr/dapr");

const express = require('express');
const bodyParser = require('body-parser');
const cors = require('cors');

const port = parseInt(process.env["PORT"] ?? "4002");
const sourceId = process.env["SOURCE_ID"];
const daprClient = new dapr.DaprClient();

async function main() {
  const app = express();
  app.use(cors());
  app.use(bodyParser.urlencoded( { extended: false }));
  app.use(bodyParser.json());

  app.post('/acquire', async (req, res, next) => {
    try {
      // Implement the bootstrap logic here

      res.status(200).json(result);   // Send the retrieved result as the response body
    } catch (err) {
      next(err);
    }
  });

  app.listen(port, () => console.log(`sourceProxy.main - Reactive Graph Source Node Proxy listening on port:${port}`));
}
```

### Source reactivator
-	The Source Reactivator is responsible for tracking and propagating any changes in the data to the subsequent Source component, the change service. You will implement the logic for tracking the changes in your source System in this component.
- The Source reactivator and the Source change service uses the [pub/sub](https://docs.dapr.io/developing-applications/building-blocks/pubsub/pubsub-overview/) feature from Dapr to transmit the events. The reactivator acts as the publisher while the change service acts as the subscriber. The pubsub name will be available on the ‘PUBSUB’ environment variable (default is ‘rg-pubsub’) and the topic name will be ‘<queryId>-change’.

#### Formatting a change event:
The code blocks below showcase how the change event should be formatted depending on the operation. For any change event, three fields are required in the json block: `op`, `payload` and `ts_ms`. 

Insert operation:
```json
{
  "op": "i",
  "payload": {
    "after": {
      "id": <id>,
      "labels": [<labels>],
      "properties": {} // Any additional data of the change event
    },
    "before": {}, // empty
    "source": {
      "db": "", 
      "lsn": "",
      "table": "", // Oneof `node` or `relation`. Use `relation` if we are working with a relational Source event
      "ts_ms": "",
      "ts_sec": ""
    }
  },
  "ts_ms": <Current time in milliseconds>
}
```
Delete Operation:
```json
{
  "op": "d",
  "payload": {
    "after": {}, // Empty
    "before": {
      "id": <id>,
      "labels": [<labels>],
      "properties": {}
    },
    "source": {
      "db": "",
      "lsn": "",
      "table": "", // Oneof `node` or `relation`
      "ts_ms": "",
      "ts_sec": ""
    }
  },
  "ts_ms": <Current time in milliseconds>
}
```

Update Operation:
```json
{
  "op": "u",
  "payload": {
    "after": {
      "id": <id>,
      "labels": [<labels>],
      "properties": {} // different across sources
    },
    "before": {
      "id": <id>,
      "labels": [<labels>],
      "properties": {
      }
    }, 
    "source": {
      "db": "", 
      "lsn": "",
      "table": "", // Oneof `node` or `relation`
      "ts_ms": "",
      "ts_sec": ""
    }
  },
  "ts_ms": <Current time in milliseconds>
}
```

#### pub/sub sample code
Sample Dapr publisher code:
```java
import com.fasterxml.jackson.databind.JsonNode;
import io.dapr.client.DaprClient;
import io.dapr.client.DaprClientBuilder;

import java.util.List;

public class DaprChangePublisher implements ChangePublisher {
    private DaprClient client;
    private String pubsubName;
    private String sourceId;

    public DaprChangePublisher(String sourceId, String pubsubName) {
        this.sourceId = sourceId;
        this.pubsubName = pubsubName;
        client = new DaprClientBuilder().build();
    }

    @Override
    public void Publish(List<JsonNode> changes) {
        client.publishEvent(pubsubName, sourceId + "-change", changes).block();
    }

    @Override
    public void close() throws Exception {
        client.close();
    }
}
```



## Registering a new Source

To add support for a new kind of source to Drasi, you must develop the services that will connect to the source and register a new Source Provider with Drasi. The Source Provider definition describes the services Drasi must run, where to get the images, and the configuration settings that are required when an instance of that Source is created

The definition for a SourceProvider has the following basic structure:

```yaml
apiVersion: v1
kind: SourceProvider
name: <name>
tag: <tag>   # Optional.
spec:
  services:
    proxy: # One of the required services.
        image: <image_name> # Required. Cannot be overwritten.
        dapr: # Optional; used for specifying dapr-related annotations
            app-port: <value> # Optional
            app-protocol: <value> # Optional
        endpoints: # Optional; used for configuring internal/external endpoints
            <endpoint_name>:
                setting: internal/external
                target: <target>  # endpoint target
            (any additional endpoints)...
        config_schema: # Optional; used for specifying any additional environment variables
            type: object
            properties: 
                <name>:
                    type: <type>  # One of [string, integer, boolean, array or object]
                    default: <value> # Optional.
                (any additioanl properties)...
            required: # Optional. List any required properties here
    reactivator: # One of the required services.
        image: <image_name> # Required. Cannot be overwritten.
        dapr: # Optional; used for specifying dapr-related annotations
            app-port: <value> # Optional
            app-protocol: <value> # Optional
        endpoints: # Optional; used for configuring internal/external endpoints
            <endpoint_name>:
                setting: internal/external
                target: <target>  # endpoint target
            (any additional endpoints)...
        config_schema: # Optional; used for specifying any additional environment variables
            type: object
            properties: 
                <name>:
                    type: <type>  # One of [string, integer, boolean, array or object]
                    default: <value> # Optional.
                (any additioanl properties)...
            required: # Optional. List any required properties here
    (any additional services)...
  config_schema: # Optional; 
                 # The environment variables defined here will be 
                 # accessible by all services
    type: object
    properties: 
        <name>:
            type: <type>  # One of [string, integer, boolean, array or object]
            default: <value> # Optional.
        (any additioanl properties)...
    required: # Optional. List any required properties here
    
```

In the SourceProvider definition:
- **apiVersion**: Must be **v1**
- **kind**: Must be **SourceProvider**
- **name**: Specifies the kind of Source that we are trying to register
- **tag**: Optional. This is used for specifying the "version" of the SourceProvider


The section below provides a more detailed walkthrough of the various fields under the `spec` section.

### Services

The `services` field configures the definition of the serivce(s) of a Source. For any SourceProvider, you must define two required services: `proxy` and `reactivator`, and you can choose to define additional services if needed. Every service will be rendered into an unique Kubernetes deployment and ultimately a Kubernetes pod. For each `service`, there are four fields that you can configure:
- `image`
  - `image` is a required field and you can specify the image to use for this source service here. 
    - (NOTE: Drasi assumes that the image lives in the same registry that you used when you executed `drasi init`).
  - `endpoints`
    - If your source has a port that needs to be exposed, you can specify them under the `endpoints` section. The `endpoints` section takes in a series of `endpoint`, which is a JSON object. Each `endpoint` object should have two properties: `setting` and `target`. `setting` can be either "internal" or "external", althrough we currently only support internal endpoints. For the `target` attribute, if the setting is set to `internal`, the `target` should be a port number.
    - Each endpoint will be rendered into a Kuberentes Service, with the value of `target` being set as the port number.
    - The following block defines a Source that will create a Kubernetes service called `<source-name>-gateway` with a port of `4318` when deployed.
      - ```yaml 
          endpoints:
            gateway:
              setting: internal
              target: "4318" 
  - `dapr`: optional. This field is used for specifying any [dapr annotation](https://docs.dapr.io/reference/arguments-annotations-overview/) that the user wishes to include. Currently we only support `app-port` and `app-protocol`. 
    - The `app-port` annotation is used to tell Dapr which port the application is listening on, whereas the `app-protocol` annotation configures the protocol that Dapr uses to communicate with your app
      - Sample yaml block:
      - ```yaml 
          dapr:
            app-port: 4002
  - `config_schema`
    - This is used for defining environment variables; however, the environment variables that are defined here are only accessible for this particular service.
    - The configurations are defined by following JSON Schema. We define this field to be of type `object`, and list all of the configs (environment variables) under the `properties` section. For each of the property, you need to specify its type and an optional default value. For any required environment variables, you can list them under the `require` section as an array of elements
    - Sample:
     ```yaml
        config_schema:
          type: object
          properties:
            foo:
              type: string
              default: bar
            property2:
              type: boolean
              default: true
          required:
            - foo


### Config Schema

The `config_schema` section that is at the same level as the `services` section is used for defining any enviroment variables that will be shared and accessible by all services. Similarly, this field can be defined in a similar way as how you would define the `config_schema` field for each service.

For example, the following section will specify two environment variables `foo` and `isTrue` for this source. `foo` is a required environment variable and it expects the input to be of type `string`, whereas `isTrue` expects the input to be of type `boolean` and is not a required value (default value is set to `true`)

```yaml
spec:
   services:
     ...
   config_schema:
      type: object
      properties:
        foo:
          type: string
        isTrue:
          type: boolean
          default: true
      required:
        - foo
```





### Validating the SourceProvider file
To validate a SourceProvider yaml file, there are two approaches:
1. Using `apply` command from the Drasi CLI. The CLI will automatically validate the SourceProvider before registering it. 
2. Using the [Drasi VSCode Extension](/solution-developer/vscode-extension/). The extension will detect all of the SourceProvider yaml files in the current workspace. Click on the `Validate` button next to each instance to validate a specific SourceProvider definition.

### Sample SourceProvider and Source file
This section contains a sample SourceProvider file and a Source file for the `Postgres` source.
The `Postgres` source:
- Contains two services: `proxy` and `reactivator`
  - `proxy` uses the image `source-sql-proxy`, and its dapr app-port should be set to `4002`.
  - `reactivator` uses the image `source-debezium-reactivator`. It has an additional environment variable with the name of `client` and a default value of `pg`.
- Has five required environment variables: database, host, port, password, user
SourceProvider file:
```yaml
apiVersion: v1
kind: SourceProvider
name: PostgreSQL
spec: 
  services:
    proxy:
      image: source-sql-proxy
      dapr:
        app-port: "4002"
    reactivator: 
      image: source-debezium-reactivator
      config_schema:
        type: object
        properties:
          client:
            type: string
            default: pg
      endpoints:
        gateway:
          setting: internal
          target: "8080"
  config_schema:
    type: object
    properties:
      database:
        type: string
      host:
        type: string
      password:
        type: string
      port:
        type: number
      ssl:
        type: boolean
        default: false
      user:
        type: string
      tables:
        type: array
    required:
      - database
      - host
      - port
      - password
      - user
      - tables
```
To register a SourceProvider file, use the `apply` command from the Drasi `CLI`: 
```bash
drasi apply -f <name-of-source-provider-file>.yaml
```

You can list all of the registered types of Source using the following command:
```bash
drasi list sourceprovider
```


To deploy a `PostgreSQL` source, we simply need to create a Source file that supplies all of the required values. In this case, we need to supply a value for all of the environment variables that are marked as required. Below is a sample Source file for deploying a `PostgreSQL` source (Notice that since we are not overwritting any service configurations, we can simply omit the `services` field in this file):
```yaml
apiVersion: v1
kind: Source
name: hello-world
spec:
  kind: PostgreSQL
  properties:
    host: postgres
    user: test
    port: 5432
    ssl: false
    password: test
    database: hello-world
    tables:
      - public.Message
```
Similarly, this source file can also be registered using the CLI:
```
drasi apply -f <name-of-the-source-file>.yaml
```