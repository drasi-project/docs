---
type: "docs"
title: "Sources"
linkTitle: "Sources"
weight: 40
description: >
    What are Sources and How to Use Them
---

Sources provide connectivity to the systems that Drasi can observe as sources of change. Sources perform three important functions within Drasi:
- Process the change log/feed generated by the source system and push those changes to each [Continuous Query](/solution-developer/components/continuous-queries) that uses that Source as input.
- Translate source change data into a consistent property graph data model so that subscribed Continuous Queries can use that data as if it where a graph of Nodes and Relations. For graph sources, such as Gremlin, no translation is necessary. But for non-graph sources, such as PostgreSQL and Kubernetes, the Source transforms the data (more detail is provided in the individual Sources sections below).
- Provide a way for Continuous Queries to query the source system at startup to initialize the state of the Continuous Query result.

{{< figure src="simple-end-to-end.png" alt="End to End" width="65%" >}}

Drasi currently provides Sources for the following source systems:

- [Azure Cosmos DB Gremlin API](#azure-cosmos-db-gremlin-api-source)
- [PostgreSQL](#postgresql-source)
- [SQL Server](#sqlserver-source)
- [Event Hubs](#event-hubs-source)
- [Dataverse](#dataverse-source)
- [Kubernetes](#kubernetes-source) (experimental)

## Creation
Sources can be created and managed using the `drasi` CLI. 

The easiest way to create a Source, and the way you will often create one as part of a broader software solution, is to:

1. Collect credentials and endpoint addresses that provide access to the change log and query API of the source system you want to connect to.
2. Create a YAML file containing the Source resource definition. This will include the configuration settings that enable the Source to connect to the source system. This file can be stored in your solution repo and versioned along with all the other solution code / resources.
3. Run `drasi apply` to apply the Source resource definition to the Kubernetes cluster where your Drasi environment is deployed.

As soon as the Source is created it will start running, monitoring its source system for changes and pushing them to subscribed Continuous Queries.

The definition for a Source has the following basic structure:

```
apiVersion: v1
kind: Source
name: <id>
spec:
  kind: <type>
  properties:
    (source kind specific fields)...
```
The following table describes these configuration settings:

|Name|Description|
|-|-|
|apiVersion|Must have the value **v1**|
|kind|Must have the value **Source**|
|name|The **id** of the Source. Must be unique within the scope of the Sources in the Drasi deployment. The  **id** is used to identify the Source through the CLI/API and in a Continuous Query definitions to identify which Sources the Continuous Query subscribes to for change events.|
|spec.kind|The type of Source to create, which defines the type of database or source system the Source connects to. Must be one of [CosmosGremlin](#azure-cosmos-db-gremlin-api-source), [PostgreSQL](#postgresql-source) or [Kubernetes](#kubernetes-source).|
|spec.properties.*|The configuration settings passed to the Source as name-value pairs. Properties differ depending on the Source type (**spec.kind**). See the individual Source sections below for the properties required by each Source type.  Any of these properties can either be specified inline or reference a secret. eg. 
```yaml
  kind: PostgreSQL
  properties:
    user: my-user
    password:
      kind: Secret
      name: pg-creds
      key: password
```

Once configured, to create a Source defined in a file called `source.yaml`, you would run the command:

```
drasi apply -f source.yaml
```

You can then use the standard `drasi` commands to query the existence and status of the Source resource. For example, to see a list of the active Sources, run the following command:

```
drasi list source
```

## Deletion
To delete an active Source, run the following command:

```
drasi delete source <id>
```

For example, if the Source ID is `human-resources`, you would run,

```
drasi delete source human-resources
```

**Note**: Drasi does not currently enforce dependency integrity between Sources and Continuous Queries. If you delete a Source that is used by one or more Continuous Queries, they will stop getting change events and stop producing results.

## Configuring Sources
The following sections describe the configuration of the Source types currently supported by Drasi.

- [Azure Cosmos DB Gremlin API](#azure-cosmos-db-gremlin-api-source)
- [PostgreSQL](#postgresql-source)
- [Kubernetes](#kubernetes-source) (experimental)

### Azure Cosmos DB Gremlin API Source

The Azure Cosmos DB Gremlin API Source enables Drasi connectivity to Azure Cosmos DB Gremlin API. It uses the Cosmos DB Change Log as the source of database change events, and calls the Gremlin API to retrieve data required to bootstrap Continuous Queries at creation.

#### Source Requirements
For the Cosmos DB Gremlin Source to function, you must ensure the Full Fidelity Change Feed support is enabled on the Cosmos Account you intend to use. Currently (as of 02/03/2023), this needs to be manually requested by [filling out this form](https://forms.office.com/pages/responsepage.aspx?id=v4j5cvGGr0GRqy180BHbR9ecQmQM5J5LlXYOPoIbyzdUOFVRNUlLUlpRV0dXMjFRNVFXMDNRRjVDNy4u).

#### Configuration Settings
The following is an example of a full resource definition for an Azure Cosmos DB Gremlin API Source using Kubernetes Secrets to securely store database credentials:

```
apiVersion: v1
kind: Source
name: retail-ops
spec:
  kind: CosmosGremlin
  properties:
    accountEndpoint: 
      kind: Secret
      name: creds
      key: account-endpoint
    database: Contoso
    container: RetailOperations
    partitionKey: name
```

> Note: You could use the following command to easily create the seret referenced here:
  ```bash
  kubectl create secret generic creds --from-literal=account-endpoint=...
  ```

In the Source resource definition:
- **apiVersion** must be **v1**
- **kind** must be **Source**
- **name** is the **id** of the Source and must be unique. This id is used in a Continuous Query definitions to identify which Sources the Continuous Query subscribes to for change events.
- **spec.kind** must be **CosmosGremlin**

The following table describes the Cosmos Gremlin specific properties that must be configured in the **spec** object:
|Property|Description|
|-|-|
|accountEndpoint|The **PRIMARY** or **SECONDARY CONNECTION STRING** from the **Keys** page of the Azure Cosmsos DB Account page of the Azure Portal.|
|database|**Database Id** from the Cosmos DB account.|
|container|**Graph Id** from the Cosmos DB Database.|
|partitionKey|The **Partition Key** configured on the **Graph**.|

#### Data Transformation
Cosmos DB Gremlin already uses a property graph data model and so the Source does not need to do any data transformation as it processes the inbound changes. The only thing to note is the terminology differences between Gremlin and Drasi summarized in this table:

|Gremlin Name|Drasi Name|
|-|-|
|Vertex|Node|
|Edge|Relation|

### PostgreSQL Source
The PostgreSQL Source enables Drasi connectivity to PostgreSQL databases. It uses the PostgreSQL replication log as the source of database change events, and calls the SQL API to retrieve data required to bootstrap Continuous Queries at creation.

#### Source Requirements

Your PostgreSQL database must be running at least version 10 and have `LOGICAL` replication enabled. See the notes on [configuring PostgreSQL replication](/reference/postgresql-replication) for assistance.

You also need a PostgreSQL user that has at least the LOGIN, REPLICATION and CREATE permissions on the database and SELECT permissions on the tables you are interested in.

#### Configuration Settings
The following is an example of a full resource definition for a PostgreSQL Source using Kubernetes Secrets to securely store database credentials:

```bash
kubectl create secret generic pg-creds --from-literal=password=my-password
```

```yaml
apiVersion: v1
kind: Source
name: phys-ops
spec:
  kind: PostgreSQL
  properties:
    host: reactive-graph.postgres.database.azure.com
    port: 5432
    user: postgres@reactive-graph
    password:
      kind: Secret
      name: pg-creds
      key: password
    database: phys-ops
    ssl: true
    tables:
      - public.Vehicle
      - public.Zone
```

In the Source resource definition:
- **apiVersion** must be **v1**
- **kind** must be **Source**
- **name** is the **id** of the Source and must be unique. This id is used in a Continuous Query definitions to identify which Sources the Continuous Query subscribes to for change events.
- **spec.kind** must be **PostgreSQL**

The following table describes the PostgrSQL specific properties:
|Property|Description|
|-|-|
|host|The **host name** of the PostgreSQL database server.|
|port|The **port** number used to communicate with the PostgreSQL database server.|
|user|The **user id** to use for authentication against the PostgreSQL database server.|
|password|The **password** for the user account specified in the **user** property.|
|database|The name of the PostgreSQL database.|
|ssl|Does the server require a secure connection, valid values are "true" or "false".|
|tables| An array of table names that the Source should process changes for. Tables must be prefixed with their schema name.|

#### Data Transformation
The PostgreSQL Source translates the relational data from change events to more closely resemble property graph data change events so that they can be processed by subscribed Continuous Queries. To achieve this, the PostgreSQL Source represents table rows as graph Nodes, as follows:
- Each row gets represented as a Node with the table columns as properties of the Node.
- The Node is assigned an id the is a composite of the table id and the row's primary key. This is Node metadata, not a property of the Node.
- The name of the table is assigned as a **Label** of the Node.

The PostgreSQL Source **does not** interpret foreign keys or joins from the relational source, instead relying on the Source Join feature provided by Continuous Queries to mimic graph-style Relations between Nodes based on the values of specified properties. See the [Source Joins](/solution-developer/components/continuous-queries/#source-subscriptions) topic in the [Continuous Queries](/solution-developer/components/continuous-queries) section for details. 


### SQLServer Source
The SQLServer Source enables Drasi connectivity to Microsoft SQL Server databases.

#### Source Requirements

Change data capture must be enabled on the database and each table you wish to observe.  See the documentation on [configuring SQL Server for CDC](/reference/setup-mssql).

{{% alert title="Note" color="warning" %}}
If the schema of your tables change after you have enabled CDC for them, you will need to refresh the capture tables.  Please see the [Debezium documentation](https://debezium.io/documentation/reference/stable/connectors/sqlserver.html#sqlserver-schema-evolution) on this issue.
{{% /alert %}}

#### Configuration Settings
The following is an example of a full resource definition for a SQLServer Source using Kubernetes Secrets to securely store database credentials:

```bash
kubectl create secret generic sql-creds --from-literal=password=my-password
```

```yaml
apiVersion: v1
kind: Source
name: phys-ops
spec:
  kind: SQLServer
  properties:
    host: drasi-sql.database.windows.net
    port: 1433
    user: drasi-user
    password:
      kind: Secret
      name: sql-creds
      key: password
    database: phys-ops
    encrypt: true
    tables:
      - dbo.Vehicle
      - dbo.Zone
```

In the Source resource definition:
- **apiVersion** must be **v1**
- **kind** must be **Source**
- **name** is the **id** of the Source and must be unique. This id is used in a Continuous Query definitions to identify which Sources the Continuous Query subscribes to for change events.
- **spec.kind** must be **SQLServer**

The following table describes the SQL Server specific properties:
|Property|Description|
|-|-|
|host|The **host name** of the database server.|
|port|The **port** number used to communicate with the database server.|
|user|The **user id** to use for authentication against the server.|
|password|The **password** for the user account specified in the **user** property.|
|database|The name of the SQL database.|
|encrypt|Does the server require a secure connection, valid values are "true" or "false".|
|tables| An array of table names that the source should process changes for. Tables must be prefixed with their schema name.|

#### Data Transformation
The SQL Source translates the relational data from change events to more closely resemble property graph data change events so that they can be processed by subscribed Continuous Queries. To achieve this, it represents table rows as graph Nodes, as follows:
- Each row gets represented as a Node with the table columns as properties of the Node.
- The Node is assigned an id the is a composite of the table id and the row's primary key. This is Node metadata, not a property of the Node.
- The name of the table is assigned as a **Label** of the Node.

The SQL Server Source **does not** interpret foreign keys or joins from the relational source, instead relying on the Source Join feature provided by Continuous Queries to mimic graph-style Relations between Nodes based on the values of specified properties. See the [Source Joins](/solution-developer/components/continuous-queries/#source-subscriptions) topic in the [Continuous Queries](/solution-developer/components/continuous-queries) section for details. 


### Event Hubs Source

The Event Hubs source enables messages streaming through Azure Event Hubs to be mapped into graph nodes that can be referenced by a continuous query.
It can observe multiple Event Hubs within the same Event Hubs namespace, each incoming message will upsert graph node that will carry the label of the Event Hub name, and be queryable from a continuous query.

#### Configuration Settings

It is best practice to store the connection string to your Event Hubs instance in a secret.

```bash
kubectl create secret generic eventhub-creds --from-literal=eventHubConnectionString=...
```

You can then reference the secret when you create an Event Hub source as follows:

```yaml
kind: Source
apiVersion: v1
name: my-source
spec:
  kind: EventHub
  properties:
    connectionString: 
      kind: Secret
      name: eventhub-creds
      key: eventHubConnectionString
    eventHubs:
      - hub1
      - hub2
    bootstrapWindow: 0
```

In the Source resource definition:
- **apiVersion** must be **v1**
- **kind** must be **Source**
- **name** is the **id** of the Source and must be unique. This id is used in a Continuous Query definitions to identify which Sources the Continuous Query subscribes to for change events.
- **spec.kind** must be **EventHub**

The following table describes the EventHub specific properties:
|Property|Description|
|-|-|
|connectionString|Connection string for the Event Hubs endpoint|
|eventHubs|A list of Event Hubs within the Event Hubs namespace to observe|
|bootstrapWindow|When a query bootstraps, it can also fetch all the messages for the previous (n) minutes.  This value defines how many minutes of backfill data to bootstrap the query with.|


### Dataverse Source

The Dataverse source enables changes to tables in Microsoft Dataverse to be mapped into graph nodes that can be referenced by a continuous query.

#### Source Requirements

##### App registration

The Dataverse source authenticates with Dataverse using OAuth, you must first register an application in your Microsoft Entra ID tenant.

Registering your application establishes a trust relationship between your app and the Microsoft identity platform. The trust is unidirectional: your app trusts the Microsoft identity platform, and not the other way around. Once created, the application object cannot be moved between different tenants.

Follow these steps to create the app registration:

1. Sign in to the Microsoft Entra admin center as at least a Cloud Application Administrator.

1. If you have access to multiple tenants, use the Settings icon  in the top menu to switch to the tenant in which you want to register the application from the Directories + subscriptions menu.

1. Browse to Identity > Applications > App registrations and select New registration.

1. Enter a display Name for your application. Users of your application might see the display name when they use the app, for example during sign-in. You can change the display name at any time and multiple app registrations can share the same name. The app registration's automatically generated Application (client) ID, not its display name, uniquely identifies your app within the identity platform.

1. Select Register to complete the initial app registration.

When registration finishes, the Microsoft Entra admin center displays the app registration's Overview pane. You see the Application (client) ID. Also called the client ID, this value uniquely identifies your application in the Microsoft identity platform.

Next, you need to add credentials to the application. Credentials allow your application to authenticate as itself, requiring no interaction from a user at runtime.

1. In the Microsoft Entra admin center, in App registrations, select your application.
1. Select Certificates & secrets > Client secrets > New client secret.
1. Add a description for your client secret.
1. Select an expiration for the secret or specify a custom lifetime.
1. Select Add.
1. Record the secret's value for use in your client application code. This secret value is never displayed again after you leave this page.

For more information see [Use OAuth authentication with Microsoft Dataverse](https://learn.microsoft.com/en-ca/power-apps/developer/data-platform/authenticate-oauth)

##### Create and bind Dataverse user account to the registered app

The first thing you must do is create a custom security role that will define what access and privileges this account will have within the Dataverse organization. More information: [Create or configure a custom security role](https://learn.microsoft.com/en-us/power-platform/admin/database-security#create-or-configure-a-custom-security-role)

After you have created the custom security role, you must create the user account which will use it.

The procedure to create this user is different from creating a licensed user. Use the following steps:

1. Navigate to Settings > Security > Users

1. In the view drop-down, select Application Users.

1. Click New. Then verify that you are using the Application user form.
If you do not see the Application ID, Application ID URI and Azure AD Object ID fields in the form, you must select the Application User form from the list.

1. Add the appropriate values to the fields:
|Field|Value|
|-|-|
|User Name|A name for the user|
|Application ID|The Application ID value for the application registered with Microsoft Entra ID.|
|Full Name|The name of your application.|
|Primary Email|The email address for the user.|

1. Associate the application user with the custom security role you created.

More information: [Manually create a Dataverse application user](https://learn.microsoft.com/en-ca/power-apps/developer/data-platform/authenticate-oauth#manually-create-a-dataverse-application-user)

##### Enable change tracking

For each table that you wish to observe, you need to enable `Track Changes` under the properties of that table in the [Power Apps builder](https://make.powerapps.com).
Also, take note of the internal system name of the tables that you wish to observe.

#### Configuration Settings

```yaml
kind: Source
apiVersion: v1
name: my-source
spec:
  kind: Dataverse
  properties:
    endpoint: https://xxxxx.api.crm4.dynamics.com/
    clientId: 00000000-0000-0000-000000000000
    secret: xxxxxx
    entities:
      - msdyn_customerasset
```

The following table describes the Dataverse specific properties:
|Property|Description|
|-|-|
|endpoint|The API endpoint for the Dataverse environment.  This can be found in the [Power Platform admin center](https://admin.powerplatform.microsoft.com/home), under Environments|
|clientId|The clientId of the app registration|
|secret|The secret under the credentials of the app registration|
|entities|A list of tables to observe.  In the form of the internal system name, visible in [Power Apps](https://make.powerapps.com)|


### Kubernetes Source
The Kubernetes Source is an early stage experimental Source that enables Drasi connectivity to Kubernetes clusters, enabling Drasi to support Continuous Queries that incorporate changes to Kubernetes resources.

#### Source Requirements

You will need a client side credentials that can be used to authenticate against your Kubernetes cluster and has permission to watch resources.

#### Configuration Settings
The following is an example of a full resource definition for a Kubernetes Source using Kubernetes Secrets to securely store credentials:

To get the credentials, export the Kubernetes credentials to a file named `credentials.yaml`

- For self hosted clusters, you can find this in your [kubeconfig](https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/) file
- For AKS, you can use this command
```
az aks get-credentials --resource-group <resource group> --name <cluster name> --file credentials.yaml
```

Create a secret named `k8s-context` from the `credentials.yaml` file

```bash
kubectl create secret generic k8s-context --from-file=credentials.yaml
```

```yaml
apiVersion: v1
kind: Source
name: k8s
spec:
  kind: Kubernetes
  properties:
    kubeconfig:
      kind: Secret
      name: k8s-context
      key: credentials.yaml
```

In the Source resource definition:
- **apiVersion** must be **v1**
- **kind** must be **Source**
- **name** is the **id** of the Source and must be unique. This id is used in a Continuous Query definitions to identify which Sources the Continuous Query subscribes to for change events.
- **spec.kind** must be **Kubernetes**

The following table describes the properties that must be configured in the **spec** object:
|Property|Description|
|-|-|
|kubeconfig|A [kubeconfig](https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig) containing the credentials to connect to your cluster|

#### Data Transformation

Currently, only Pods and Containers are projected onto a graph schema.  The relation between them is labeled `HOSTS` and flows from the `Pod` to the `Container`.  A MATCH cypher clause that connects these would look as follows

```cypher
MATCH (p:Pod)-[:HOSTS]->(c:Container) 
```

The following properties are projected to the graph nodes

|Node Label|Property|Origin|
|-|-|-|
|Container|name|Pod.status.containerStatuses[].name|
|Container|image|Pod.status.containerStatuses[].image|
|Container|started|Pod.status.containerStatuses[].started|
|Container|ready|Pod.status.containerStatuses[].ready|
|Container|restartCount|Pod.status.containerStatuses[].restartCount|
|Container|state|Pod.status.containerStatuses[].state|
|Container|message|Pod.status.containerStatuses[].state.message|
|Container|reason|Pod.status.containerStatuses[].state.reason|
|Container|terminationMessage|Pod.status.containerStatuses[].lastState.terminated.message|
|Pod|name|Pod.metadata.name|
|Pod|podIP|Pod.status.podIP|
|Pod|phase|Pod.status.phase|
|Pod|message|Pod.status.message|
|Pod|hostIP|Pod.status.hostIP|
|Pod|reason|Pod.status.reason|


## Creating the Source proxy and reactivator
{{< figure src="source.jpg" alt="Source" width="65%" >}}
### Source proxy
-	The Source proxy is responsible for bootstrapping the initial data when a continuous query is deployed. 
-	For any source proxy, we need to set up an HTTP server that listens for a POST request on the route ‘/acquire’. The bootstrap logic will be implemented under this section. As a result, when a POST request is received, the initial data will be obtained based on the logic here. 
    - For example, the SQL proxy uses knex to query and retrieve all of the rows in a SQL table and use the data as the bootstrapping data.
    - It is also worth noting that the passthru proxy might be useful in certain situations. The passthru proxy handles the bootstrapping process by using dapr to invoke a HTTP Post method called “acquire”, which is defined in the Source reactivator.

Sample code block:
```js
require("dotenv").config();

const fs = require('fs');
const dapr =  require("@dapr/dapr");

const express = require('express');
const bodyParser = require('body-parser');
const cors = require('cors');

const port = parseInt(process.env["PORT"] ?? "4002");
const sourceId = process.env["SOURCE_ID"];
const daprClient = new dapr.DaprClient();

async function main() {
  const app = express();
  app.use(cors());
  app.use(bodyParser.urlencoded( { extended: false }));
  app.use(bodyParser.json());

  app.post('/acquire', async (req, res, next) => {
    try {
      // Implement the bootstrap logic here

      res.status(200).json(result);   // Send the retrieved result as the response body
    } catch (err) {
      next(err);
    }
  });

  app.listen(port, () => console.log(`sourceProxy.main - Reactive Graph Source Node Proxy listening on port:${port}`));
}
```

### Source reactivator
-	The Source Reactivator is responsible for tracking and propagating any changes in the data to the subsequent Source component, the change service. You will implement the logic for tracking the changes in your source System in this component.
- The Source reactivator and the Source change service uses the [pub/sub](https://docs.dapr.io/developing-applications/building-blocks/pubsub/pubsub-overview/) feature from Dapr to transmit the events. The reactivator acts as the publisher while the change service acts as the subscriber. The pubsub name will be available on the ‘PUBSUB’ environment variable (default is ‘rg-pubsub’) and the topic name will be ‘<queryId>-change’.

#### Formatting a change event:
The code blocks below showcase how the change event should be formatted depending on the operation. For any change event, three fields are required in the json block: `op`, `payload` and `ts_ms`. 

Insert operation:
```json
{
  "op": "i",
  "payload": {
    "after": {
      "id": <id>,
      "labels": [<labels>],
      "properties": {} // Any additional data of the change event
    },
    "before": {}, // empty
    "source": {
      "db": "", 
      "lsn": "",
      "table": "", // Oneof `node` or `relation`. Use `relation` if we are working with a relational Source event
      "ts_ms": "",
      "ts_sec": ""
    }
  },
  "ts_ms": <Current time in milliseconds>
}
```
Delete Operation:
```json
{
  "op": "d",
  "payload": {
    "after": {}, // Empty
    "before": {
      "id": <id>,
      "labels": [<labels>],
      "properties": {}
    },
    "source": {
      "db": "",
      "lsn": "",
      "table": "", // Oneof `node` or `relation`
      "ts_ms": "",
      "ts_sec": ""
    }
  },
  "ts_ms": <Current time in milliseconds>
}
```

Update Operation:
```json
{
  "op": "u",
  "payload": {
    "after": {
      "id": <id>,
      "labels": [<labels>],
      "properties": {} // different across sources
    },
    "before": {
      "id": <id>,
      "labels": [<labels>],
      "properties": {
      }
    }, 
    "source": {
      "db": "", 
      "lsn": "",
      "table": "", // Oneof `node` or `relation`
      "ts_ms": "",
      "ts_sec": ""
    }
  },
  "ts_ms": <Current time in milliseconds>
}
```

#### pub/sub sample code
Sample Dapr publisher code:
```java
import com.fasterxml.jackson.databind.JsonNode;
import io.dapr.client.DaprClient;
import io.dapr.client.DaprClientBuilder;

import java.util.List;

public class DaprChangePublisher implements ChangePublisher {
    private DaprClient client;
    private String pubsubName;
    private String sourceId;

    public DaprChangePublisher(String sourceId, String pubsubName) {
        this.sourceId = sourceId;
        this.pubsubName = pubsubName;
        client = new DaprClientBuilder().build();
    }

    @Override
    public void Publish(List<JsonNode> changes) {
        client.publishEvent(pubsubName, sourceId + "-change", changes).block();
    }

    @Override
    public void close() throws Exception {
        client.close();
    }
}
```



## Registering a new Source

To add support for a new kind of source to Drasi, you must develop the services that will connect to the source and register a new Source Provider with Drasi. The Source Provider definition describes the services Drasi must run, where to get the images, and the configuration settings that are required when an instance of that Source is created

The definition for a SourceProvider has the following basic structure:

```yaml
apiVersion: v1
kind: SourceProvider
name: <name>
tag: <tag>   # Optional.
spec:
  services:
    proxy: # One of the required services.
        image: <image_name> # Required. Cannot be overwritten.
        dapr: # Optional; used for specifying dapr-related annotations
            app-port: <value> # Optional
            app-protocol: <value> # Optional
        endpoints: # Optional; used for configuring internal/external endpoints
            <endpoint_name>:
                setting: internal/external
                target: <target>  # endpoint target
            (any additional endpoints)...
        config_schema: # Optional; used for specifying any additional environment variables
            type: object
            properties: 
                <name>:
                    type: <type>  # One of [string, integer, boolean, array or object]
                    default: <value> # Optional.
                (any additioanl properties)...
            required: # Optional. List any required properties here
    reactivator: # One of the required services.
        image: <image_name> # Required. Cannot be overwritten.
        dapr: # Optional; used for specifying dapr-related annotations
            app-port: <value> # Optional
            app-protocol: <value> # Optional
        endpoints: # Optional; used for configuring internal/external endpoints
            <endpoint_name>:
                setting: internal/external
                target: <target>  # endpoint target
            (any additional endpoints)...
        config_schema: # Optional; used for specifying any additional environment variables
            type: object
            properties: 
                <name>:
                    type: <type>  # One of [string, integer, boolean, array or object]
                    default: <value> # Optional.
                (any additioanl properties)...
            required: # Optional. List any required properties here
    (any additional services)...
  config_schema: # Optional; 
                 # The environment variables defined here will be 
                 # accessible by all services
    type: object
    properties: 
        <name>:
            type: <type>  # One of [string, integer, boolean, array or object]
            default: <value> # Optional.
        (any additioanl properties)...
    required: # Optional. List any required properties here
    
```

In the SourceProvider definition:
- **apiVersion**: Must be **v1**
- **kind**: Must be **SourceProvider**
- **name**: Specifies the kind of Source that we are trying to register
- **tag**: Optional. This is used for specifying the "version" of the SourceProvider


The section below provides a more detailed walkthrough of the various fields under the `spec` section.

### Services

The `services` field configures the definition of the serivce(s) of a Source. For any SourceProvider, you must define two required services: `proxy` and `reactivator`, and you can choose to define additional services if needed. Every service will be rendered into an unique Kubernetes deployment and ultimately a Kubernetes pod. For each `service`, there are four fields that you can configure:
- `image`
  - `image` is a required field and you can specify the image to use for this source service here. 
    - (NOTE: Drasi assumes that the image lives in the same registry that you used when you executed `drasi init`).
  - `endpoints`
    - If your source has a port that needs to be exposed, you can specify them under the `endpoints` section. The `endpoints` section takes in a series of `endpoint`, which is a JSON object. Each `endpoint` object should have two properties: `setting` and `target`. `setting` can be either "internal" or "external", althrough we currently only support internal endpoints. For the `target` attribute, if the setting is set to `internal`, the `target` should be a port number.
    - Each endpoint will be rendered into a Kuberentes Service, with the value of `target` being set as the port number.
    - The following block defines a Source that will create a Kubernetes service called `<source-name>-gateway` with a port of `4318` when deployed.
      - ```yaml 
          endpoints:
            gateway:
              setting: internal
              target: "4318" 
  - `dapr`: optional. This field is used for specifying any [dapr annotation](https://docs.dapr.io/reference/arguments-annotations-overview/) that the user wishes to include. Currently we only support `app-port` and `app-protocol`. 
    - The `app-port` annotation is used to tell Dapr which port the application is listening on, whereas the `app-protocol` annotation configures the protocol that Dapr uses to communicate with your app
      - Sample yaml block:
      - ```yaml 
          dapr:
            app-port: 4002
  - `config_schema`
    - This is used for defining environment variables; however, the environment variables that are defined here are only accessible for this particular service.
    - The configurations are defined by following JSON Schema. We define this field to be of type `object`, and list all of the configs (environment variables) under the `properties` section. For each of the property, you need to specify its type and an optional default value. For any required environment variables, you can list them under the `require` section as an array of elements
    - Sample:
     ```yaml
        config_schema:
          type: object
          properties:
            foo:
              type: string
              default: bar
            property2:
              type: boolean
              default: true
          required:
            - foo


### Config Schema

The `config_schema` section that is at the same level as the `services` section is used for defining any enviroment variables that will be shared and accessible by all services. Similarly, this field can be defined in a similar way as how you would define the `config_schema` field for each service.

For example, the following section will specify two environment variables `foo` and `isTrue` for this source. `foo` is a required environment variable and it expects the input to be of type `string`, whereas `isTrue` expects the input to be of type `boolean` and is not a required value (default value is set to `true`)

```yaml
spec:
   services:
     ...
   config_schema:
      type: object
      properties:
        foo:
          type: string
        isTrue:
          type: boolean
          default: true
      required:
        - foo
```





### Validating the SourceProvider file
To validate a SourceProvider yaml file, there are two approaches:
1. Using `apply` command from the Drasi CLI. The CLI will automatically validate the SourceProvider before registering it. 
2. Using the [Drasi VSCode Extension](/solution-developer/vscode-extension/). The extension will detect all of the SourceProvider yaml files in the current workspace. Click on the `Validate` button next to each instance to validate a specific SourceProvider definition.

### Sample SourceProvider and Source file
This section contains a sample SourceProvider file and a Source file for the `Postgres` source.
The `Postgres` source:
- Contains two services: `proxy` and `reactivator`
  - `proxy` uses the image `source-sql-proxy`, and its dapr app-port should be set to `4002`.
  - `reactivator` uses the image `source-debezium-reactivator`. It has an additional environment variable with the name of `client` and a default value of `pg`.
- Has five required environment variables: database, host, port, password, user
SourceProvider file:
```yaml
apiVersion: v1
kind: SourceProvider
name: PostgreSQL
spec: 
  services:
    proxy:
      image: source-sql-proxy
      dapr:
        app-port: "4002"
    reactivator: 
      image: source-debezium-reactivator
      config_schema:
        type: object
        properties:
          client:
            type: string
            default: pg
      endpoints:
        gateway:
          setting: internal
          target: "8080"
  config_schema:
    type: object
    properties:
      database:
        type: string
      host:
        type: string
      password:
        type: string
      port:
        type: number
      ssl:
        type: boolean
        default: false
      user:
        type: string
      tables:
        type: array
    required:
      - database
      - host
      - port
      - password
      - user
      - tables
```
To register a SourceProvider file, use the `apply` command from the Drasi `CLI`: 
```bash
drasi apply -f <name-of-source-provider-file>.yaml
```

You can list all of the registered types of Source using the following command:
```bash
drasi list sourceprovider
```


To deploy a `PostgreSQL` source, we simply need to create a Source file that supplies all of the required values. In this case, we need to supply a value for all of the environment variables that are marked as required. Below is a sample Source file for deploying a `PostgreSQL` source (Notice that since we are not overwritting any service configurations, we can simply omit the `services` field in this file):
```yaml
apiVersion: v1
kind: Source
name: hello-world
spec:
  kind: PostgreSQL
  properties:
    host: postgres
    user: test
    port: 5432
    ssl: false
    password: test
    database: hello-world
    tables:
      - public.Message
```
Similarly, this source file can also be registered using the CLI:
```
drasi apply -f <name-of-the-source-file>.yaml
```